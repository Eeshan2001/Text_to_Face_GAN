{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwCY10lu3Ok5"
      },
      "outputs": [],
      "source": [
        "print(\"Installing CLIP...\")\n",
        "!git clone https://github.com/openai/CLIP                 &> /dev/null\n",
        " \n",
        "print(\"Installing Python Libraries for AI...\")\n",
        "!git clone https://github.com/CompVis/taming-transformers &> /dev/null\n",
        "!pip install transformers                                 &> /dev/null\n",
        "!pip install torch==1.13.0                                &> /dev/null\n",
        "!pip install torchvision=0.14.0                           &> /dev/null\n",
        "!pip install torchtext==0.14.0                            &> /dev/null\n",
        "!pip install torchaudio==0.13.0                           &> /dev/null\n",
        "# The transformer is a component used in many neural network designs for processing long sequential data, such as natural language text, genome sequences, sound signals or time series data                          \n",
        "!pip install ftfy regex tqdm omegaconf pytorch-lightning  &> /dev/null\n",
        "!pip install kornia                                       &> /dev/null\n",
        "!pip install einops                                       &> /dev/null\n",
        "!pip install wget                                         &> /dev/null\n",
        "# !pip install taming-transformers-rom1504\n",
        "!pip install fastcore -U                                  &> /dev/null\n",
        "\n",
        "!pip install stegano                                      &> /dev/null\n",
        "!apt install exempi                                       &> /dev/null\n",
        "!pip install python-xmp-toolkit                           &> /dev/null\n",
        "!pip install imgtag                                       &> /dev/null\n",
        "!pip install pillow==7.1.2                                &> /dev/null \n",
        "# The Pillow library contains all the basic image processing functionality. You can do image resizing, rotation and transformation                             "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b31bg5oR3rky"
      },
      "outputs": [],
      "source": [
        "!curl -L -o celebahq.yaml -C - 'https://app.koofr.net/content/links/6dddf083-40c8-470a-9360-a9dab2a94e96/files/get/2021-04-23T18-11-19-project.yaml?path=%2F2021-04-23T18-11-19_celebahq_transformer%2Fconfigs%2F2021-04-23T18-11-19-project.yaml&force' #CelebA-HQ\n",
        "!curl -L -o celebahq.ckpt -C - 'https://app.koofr.net/content/links/6dddf083-40c8-470a-9360-a9dab2a94e96/files/get/last.ckpt?path=%2F2021-04-23T18-11-19_celebahq_transformer%2Fcheckpoints%2Flast.ckpt&force' #CelebA-HQ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUkl2NZM3tm-"
      },
      "outputs": [],
      "source": [
        "# Loading libraries and definitions\n",
        "# from pytorch_lightning.utilities.rank_zero import rank_zero_only\n",
        "\n",
        "#taming>data>utils.py -- delete 'from torch._six' and interchange string_classes with str\n",
        "\n",
        "import argparse\n",
        "import math\n",
        "from pathlib import Path\n",
        "import sys\n",
        " \n",
        "sys.path.append('./taming-transformers')\n",
        "from IPython import display\n",
        "from base64 import b64encode\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from taming.models import cond_transformer, vqgan\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import functional as TF\n",
        "from tqdm.notebook import tqdm\n",
        " \n",
        "from CLIP import clip\n",
        "import kornia.augmentation as K\n",
        "import numpy as np\n",
        "import imageio\n",
        "from PIL import ImageFile, Image\n",
        "from imgtag import ImgTag    # metadatos \n",
        "import libxmp                # metadatos\n",
        "from libxmp import *         # metadatos\n",
        "from libxmp import consts\n",
        "from stegano import lsb\n",
        "import json\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        " \n",
        "def sinc(x):\n",
        "    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n",
        "  \n",
        "def lanczos(x, a):\n",
        "    cond = torch.logical_and(-a < x, x < a)\n",
        "    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n",
        "    return out / out.sum()\n",
        " \n",
        " \n",
        "def ramp(ratio, width):\n",
        "    n = math.ceil(width / ratio + 1)\n",
        "    out = torch.empty([n])\n",
        "    cur = 0\n",
        "    for i in range(out.shape[0]):\n",
        "        out[i] = cur\n",
        "        cur += ratio\n",
        "    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n",
        " \n",
        " \n",
        "def resample(input, size, align_corners=True):\n",
        "    n, c, h, w = input.shape\n",
        "    dh, dw = size\n",
        " \n",
        "    input = input.view([n * c, 1, h, w])\n",
        " \n",
        "    if dh < h:\n",
        "        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n",
        "        pad_h = (kernel_h.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n",
        "        input = F.conv2d(input, kernel_h[None, None, :, None])\n",
        " \n",
        "    if dw < w:\n",
        "        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n",
        "        pad_w = (kernel_w.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n",
        "        input = F.conv2d(input, kernel_w[None, None, None, :])\n",
        " \n",
        "    input = input.view([n, c, h, w])\n",
        "    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n",
        " \n",
        " \n",
        "class ReplaceGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x_forward, x_backward):\n",
        "        ctx.shape = x_backward.shape\n",
        "        return x_forward\n",
        " \n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        return None, grad_in.sum_to_size(ctx.shape)\n",
        " \n",
        " \n",
        "replace_grad = ReplaceGrad.apply\n",
        " \n",
        " \n",
        "class ClampWithGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, min, max):\n",
        "        ctx.min = min\n",
        "        ctx.max = max\n",
        "        ctx.save_for_backward(input)\n",
        "        return input.clamp(min, max)\n",
        " \n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        input, = ctx.saved_tensors\n",
        "        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\n",
        " \n",
        " \n",
        "clamp_with_grad = ClampWithGrad.apply\n",
        " \n",
        " \n",
        "def vector_quantize(x, codebook):\n",
        "    d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) - 2 * x @ codebook.T\n",
        "    indices = d.argmin(-1)\n",
        "    x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype) @ codebook\n",
        "    return replace_grad(x_q, x)\n",
        " \n",
        " \n",
        "\n",
        "# Here we calculate how similar image and text.\n",
        "class Prompt(nn.Module):\n",
        "    def __init__(self, embed, weight=1., stop=float('-inf')):\n",
        "        super().__init__()\n",
        "        self.register_buffer('embed', embed)\n",
        "        self.register_buffer('weight', torch.as_tensor(weight))\n",
        "        self.register_buffer('stop', torch.as_tensor(stop))\n",
        "\n",
        "#input - batch of image cuttouts \n",
        "    def forward(self, input):\n",
        "        input_normed = F.normalize(input.unsqueeze(1), dim=2)\n",
        "        embed_normed = F.normalize(self.embed.unsqueeze(0), dim=2)\n",
        "        dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n",
        "        dists = dists * self.weight.sign()\n",
        "        return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean()\n",
        " \n",
        " \n",
        "def parse_prompt(prompt):\n",
        "    vals = prompt.rsplit(':', 2)\n",
        "    vals = vals + ['', '1', '-inf'][len(vals):]\n",
        "    return vals[0], float(vals[1]), float(vals[2])\n",
        " \n",
        "\n",
        "class MakeCutouts(nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "        self.augs = nn.Sequential(\n",
        "            K.RandomHorizontalFlip(p=0.5),\n",
        "            # K.RandomSolarize(0.01, 0.01, p=0.7),\n",
        "            K.RandomSharpness(0.3,p=0.4),\n",
        "            K.RandomAffine(degrees=30, translate=0.1, p=0.8, padding_mode='border'),\n",
        "            K.RandomPerspective(0.2,p=0.4),\n",
        "            K.ColorJitter(hue=0.01, saturation=0.01, p=0.7))\n",
        "        self.noise_fac = 0.1\n",
        " \n",
        " \n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        for _ in range(self.cutn):\n",
        "            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "            offsety = torch.randint(0, sideY - size + 1, ())\n",
        "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "            cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
        "        batch = self.augs(torch.cat(cutouts, dim=0))\n",
        "        if self.noise_fac:\n",
        "            facs = batch.new_empty([self.cutn, 1, 1, 1]).uniform_(0, self.noise_fac)\n",
        "            batch = batch + facs * torch.randn_like(batch)\n",
        "        return batch\n",
        " \n",
        " \n",
        "def load_vqgan_model(config_path, checkpoint_path):\n",
        "    config = OmegaConf.load(config_path)\n",
        "    if config.model.target == 'taming.models.cond_transformer.Net2NetTransformer':\n",
        "        parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\n",
        "        parent_model.eval().requires_grad_(False)\n",
        "        parent_model.init_from_ckpt(checkpoint_path)\n",
        "        model = parent_model.first_stage_model\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f'unknown model type: {config.model.target}')\n",
        "    del model.loss\n",
        "    return model\n",
        " \n",
        " \n",
        "def resize_image(image, out_size):\n",
        "    ratio = image.size[0] / image.size[1]\n",
        "    area = min(image.size[0] * image.size[1], out_size[0] * out_size[1])\n",
        "    size = round((area * ratio)**0.5), round((area / ratio)**0.5)\n",
        "    return image.resize(size, Image.LANCZOS)\n",
        "\n",
        "def download_img(img_url):\n",
        "    try:\n",
        "        return wget.download(img_url,out=\"input.jpg\")\n",
        "    except:\n",
        "        return\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrG1vRLQ57Nu"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "lossArray = []\n",
        "def startProcessing(text):\n",
        "  texts=[text]\n",
        "  target_images = []\n",
        "  is_gumbel = False\n",
        "  name_model =\"CelebA-HQ\"\n",
        "  input_images=False\n",
        "\n",
        "  max_iterations=10000\n",
        " \n",
        "\n",
        "# Parameters\n",
        "  args = argparse.Namespace(\n",
        "      prompts=texts,\n",
        "      image_prompts=target_images,\n",
        "      noise_prompt_seeds=[],\n",
        "      noise_prompt_weights=[],\n",
        "      size=[300, 300],\n",
        "      init_image=None,\n",
        "      init_weight=0.,\n",
        "      clip_model='ViT-B/32',   #ViT-L/14\n",
        "      vqgan_config='celebahq.yaml',\n",
        "      vqgan_checkpoint='celebahq.ckpt',\n",
        "      step_size=0.1,\n",
        "      cutn=64,\n",
        "      cut_pow=1.,\n",
        "      display_freq=1,\n",
        "      seed=-1,\n",
        "  )\n",
        "  # Start Processing \n",
        "\n",
        "  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "  print('Using device:', device)\n",
        "  if texts:\n",
        "      print('Using texts:', texts)\n",
        "  if target_images:\n",
        "      print('Using image prompts:', target_images)\n",
        "  if args.seed is None:\n",
        "      seed = torch.seed()\n",
        "  else:\n",
        "      seed = args.seed\n",
        "  torch.manual_seed(seed)\n",
        "  print('Using seed:', seed)\n",
        "\n",
        "  model = load_vqgan_model(args.vqgan_config, args.vqgan_checkpoint).to(device)\n",
        "  perceptor = clip.load(args.clip_model, jit=False)[0].eval().requires_grad_(False).to(device)\n",
        "\n",
        "  cut_size = perceptor.visual.input_resolution\n",
        "  if is_gumbel:\n",
        "      e_dim = model.quantize.embedding_dim\n",
        "  else:\n",
        "      e_dim = model.quantize.e_dim\n",
        "\n",
        "  f = 2**(model.decoder.num_resolutions - 1)\n",
        "  make_cutouts = MakeCutouts(cut_size, args.cutn, cut_pow=args.cut_pow)\n",
        "  if is_gumbel:\n",
        "      n_toks = model.quantize.n_embed\n",
        "  else:\n",
        "      n_toks = model.quantize.n_e\n",
        "\n",
        "  toksX, toksY = args.size[0] // f, args.size[1] // f\n",
        "  sideX, sideY = toksX * f, toksY * f\n",
        "  if is_gumbel:\n",
        "      z_min = model.quantize.embed.weight.min(dim=0).values[None, :, None, None]\n",
        "      z_max = model.quantize.embed.weight.max(dim=0).values[None, :, None, None]\n",
        "  else:\n",
        "      z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n",
        "      z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n",
        "# z : initial VQGAN-encoded image vector\n",
        "  if args.init_image:\n",
        "      pil_image = Image.open(args.init_image).convert('RGB')\n",
        "      pil_image = pil_image.resize((sideX, sideY), Image.LANCZOS)\n",
        "      z, *_ = model.encode(TF.to_tensor(pil_image).to(device).unsqueeze(0) * 2 - 1)\n",
        "  else:\n",
        "      one_hot = F.one_hot(torch.randint(n_toks, [toksY * toksX], device=device), n_toks).float()\n",
        "      if is_gumbel:\n",
        "          z = one_hot @ model.quantize.embed.weight\n",
        "      else:\n",
        "          z = one_hot @ model.quantize.embedding.weight\n",
        "      z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2)\n",
        "  z_orig = z.clone()\n",
        "  z.requires_grad_(True)\n",
        "  opt = optim.Adam([z], lr=args.step_size)\n",
        "\n",
        "  normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                  std=[0.26862954, 0.26130258, 0.27577711])\n",
        "\n",
        "  pMs = []\n",
        "\n",
        "  for prompt in args.prompts:\n",
        "      txt, weight, stop = parse_prompt(prompt)\n",
        "      embed = perceptor.encode_text(clip.tokenize(txt).to(device)).float()\n",
        "      tempans = Prompt(embed, weight, stop)\n",
        "      pMs.append(tempans.to(device))\n",
        "\n",
        "  for prompt in args.image_prompts:\n",
        "      path, weight, stop = parse_prompt(prompt)\n",
        "      img = resize_image(Image.open(path).convert('RGB'), (sideX, sideY))\n",
        "      batch = make_cutouts(TF.to_tensor(img).unsqueeze(0).to(device))\n",
        "      embed = perceptor.encode_image(normalize(batch)).float()\n",
        "      pMs.append(Prompt(embed, weight, stop).to(device))\n",
        "\n",
        "  for seed, weight in zip(args.noise_prompt_seeds, args.noise_prompt_weights):\n",
        "      gen = torch.Generator().manual_seed(seed)\n",
        "      embed = torch.empty([1, perceptor.visual.output_dim]).normal_(generator=gen)\n",
        "      pMs.append(Prompt(embed, weight).to(device))\n",
        "\n",
        "\n",
        "  def synth(z):\n",
        "      if is_gumbel:\n",
        "          z_q = vector_quantize(z.movedim(1, 3), model.quantize.embed.weight).movedim(3, 1)\n",
        "      else:\n",
        "          z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n",
        "      \n",
        "      return clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n",
        "\n",
        "  def add_stegano_data(filename):\n",
        "      data = {\n",
        "          \"title\": \" | \".join(args.prompts) if args.prompts else None,\n",
        "          \"notebook\": \"VQGAN+CLIP\",\n",
        "          \"i\": i,\n",
        "          \"model\": name_model,\n",
        "          \"seed\": str(seed),\n",
        "          \"input_images\": input_images\n",
        "      }\n",
        "      lsb.hide(filename, json.dumps(data)).save(filename)\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def checkin(i, losses):\n",
        "      losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n",
        "      lossArray.append(sum(losses).item())\n",
        "      tqdm.write(f'i: {i}, loss: {sum(losses).item():g}, losses: {losses_str}')\n",
        "      out = synth(z)\n",
        "      TF.to_pil_image(out[0].cpu()).save('progress.png')\n",
        "      add_stegano_data('progress.png')\n",
        "      # display.display(display.Image('progress.png'))\n",
        "\n",
        "# return array of losses per prompt\n",
        "  def ascend_txt(i):\n",
        "      out = synth(z) # image synthesizing(generation)\n",
        "      iii = perceptor.encode_image(normalize(make_cutouts(out))).float() # creating cutouts and encodeding them with CLIP\n",
        "\n",
        "      result = []\n",
        "\n",
        "      if args.init_weight:\n",
        "          result.append(F.mse_loss(z, z_orig) * args.init_weight / 2)\n",
        "      for prompt in pMs:\n",
        "        result.append(prompt(iii))\n",
        "      img = np.array(out.mul(255).clamp(0, 255)[0].cpu().detach().numpy().astype(np.uint8))[:,:,:]\n",
        "      img = np.transpose(img, (1, 2, 0))\n",
        "      filename = f\"steps/{i:04}.png\"\n",
        "      imageio.imwrite(filename, np.array(img))\n",
        "      add_stegano_data(filename)\n",
        "      return result\n",
        "\n",
        "  def train(i):\n",
        "      opt.zero_grad()  #restarts looping without losses from the last step by making the gradient zero at each iteration.\n",
        "      lossAll = ascend_txt(i) # calculate the loss\n",
        "      if i % args.display_freq == 0:\n",
        "          checkin(i, lossAll)\n",
        "      loss = sum(lossAll) # we get loss(array of tensors) of each prompt so we can sum them and use backward once\n",
        "      loss.backward()\n",
        "      opt.step() # updating the image vector z.\n",
        "      with torch.no_grad():\n",
        "          z.copy_(z.maximum(z_min).minimum(z_max))\n",
        "\n",
        "  i=0\n",
        "  while True:\n",
        "    # print('Before training', i)\n",
        "    train(i)\n",
        "    # print('After training', i)\n",
        "    if i == max_iterations:\n",
        "        break\n",
        "    i += 1\n",
        "  # return getImagesOutput(count)\n",
        "  # i=0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QEWZRcl-Iw0"
      },
      "outputs": [],
      "source": [
        "!mkdir steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2Rru9WjgfeT"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "shutil.rmtree('steps', ignore_errors=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQLmBSv46FHp"
      },
      "outputs": [],
      "source": [
        "def getImagesOutput(count):\n",
        "    max_iterations=100\n",
        "    print(\"Getting Imges\\n\")\n",
        "    ind=0;\n",
        "    images=[]\n",
        "    while(count):\n",
        "      filename = f\"/content/steps/{(max_iterations-count):04}.png\"\n",
        "      with open(filename, \"rb\") as img_file:\n",
        "        images.append(base64.b64encode(img_file.read()))\n",
        "      count=count-1\n",
        "    print(len(images))\n",
        "    return images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z70jp8RDUW_5"
      },
      "source": [
        "The woman has oval face and high cheekbones. She has straight hair which is brown in color. She has arched eyebrows. The smiling young attractive woman has heavy makeup. She is wearing earrings and lipstick.\n",
        "\n",
        "\n",
        "\n",
        "The woman has oval face and high cheekbones. She is young women and dark skin color. She has blonde straight hair. She is smiling and wear heavy makeup.\n",
        "\n",
        "The man has round flat and high cheekbones. He is a young man and has pale skin color. He has black hairs and smaller eyes.\n",
        "\n",
        "round cheeks, small chins\n",
        "\n",
        "An average Indian boy, 18 years old, has a round face with dark hair and brown eyes. He has a straight nose and a light to medium skin tone with minimal facial hair\n",
        "\n",
        "A young boy with White Skin, curly blonde hair, bright blue eyes, and a narrow, pointed chin.\n",
        "\n",
        "\n",
        "\n",
        "-------------------------------------------------------------\n",
        "\n",
        "Male, Female.\n",
        "\n",
        "Arched eyebrows, Bushy eyebrows, Normal eyebrows,\n",
        "\n",
        "Narrow eyes, Normal eyes,\n",
        "\n",
        "Big nose, Pointy nose, Normal nose,\n",
        "\n",
        "Big lips, Normal lips,\n",
        "\n",
        "Black hair, Blond hair, Brown hair, Gray hair,\n",
        "\n",
        "Straight hair, Wavy hair, Receding hairline, Bald,\n",
        "\n",
        "Mustache, No mustache,\n",
        "\n",
        "Five-o-clock shadow, Goatee, Sideburns, No beard,\n",
        "\n",
        "Fair, No fair,\n",
        "\n",
        "Bags under eyes, No bags under eyes,\n",
        "\n",
        "Bangs, No bangs,\n",
        "\n",
        "Chubby, No chubby,\n",
        "\n",
        "Double chin, No double chin,\n",
        "\n",
        "High cheekbones, No high cheekbones,\n",
        "\n",
        "Rosy cheeks, No rosy cheeks,\n",
        "\n",
        "Oval face, No oval face,\n",
        "\n",
        "Pale skin, Normal skin,\n",
        "\n",
        "Wearing earrings, No wearing earrings,\n",
        "\n",
        "Wearing lipstick, No wearing lipstick,\n",
        "\n",
        "Wearing eye glass, No wearing eye glassâ€™,\n",
        "\n",
        "Heavy makeup, No heavy makeup, \n",
        "\n",
        "Young, Old.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pqyx0Qh0ISz"
      },
      "source": [
        "#**Before running the cell below create a steps folder and check whether the lossArray is empty or not.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lB3vEMrd7RE"
      },
      "outputs": [],
      "source": [
        "# json_data = request.get_json(force=True)\n",
        "text_prompt =  \"An elderly man with a thin, wispy mustache, a serene expression, and deep wrinkles around the eyes.\"\n",
        "startProcessing(text_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lmk4VsplF2h4"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JE77QB1ZFuib"
      },
      "outputs": [],
      "source": [
        "ys = []\n",
        "no = 0\n",
        "li = 25\n",
        "fig, ax = plt.subplots(figsize=(12,8))\n",
        "for i in lossArray:\n",
        "  if no % li == 0:\n",
        "    ys.append(i)\n",
        "  no+=1\n",
        "print(ys)\n",
        "xs = []\n",
        "no = 0\n",
        "while len(xs) < len(ys):\n",
        "  xs.append(no)\n",
        "  no += li\n",
        "plt.xticks(range(0, len(lossArray), li))\n",
        "plt.yticks([])\n",
        "plt.ylabel('LOSS')\n",
        "plt.xlabel('ITERATIONS')\n",
        "plt.plot(xs, ys)\n",
        "for index in range(len(xs)):\n",
        "  ax.text(xs[index], ys[index], round(ys[index], 3))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3F4VwJWoGV4V"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "count = 0\n",
        "images1 = []\n",
        "imgno = 0\n",
        "for filename in os.listdir(\"steps/\"):\n",
        "  if imgno % 10 == 0:\n",
        "    img_path = f\"/content/steps/{(imgno):04}.png\"\n",
        "    images1.append(mpimg.imread(img_path)) #read an image from file into array. Returns the image data\n",
        "  imgno+=1\n",
        "print(len(images1))\n",
        "\n",
        "plt.figure(figsize=(40,30))\n",
        "columns = 11\n",
        "for i, image in enumerate(images1):\n",
        "    plt.subplot(int(len(images1) / columns + 4), columns, i + 1)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Otn0sFTbnkNP"
      },
      "source": [
        "Store the images into google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QM0vXmqhegZw",
        "outputId": "21258038-2f21-472c-b53d-e2277afafd72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZ-qQYyulqnw"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/steps/ /content/drive/MyDrive/GANRES/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUWfAVvKtAz4"
      },
      "outputs": [],
      "source": [
        "!pip install pipreqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kln9s01wtV8r"
      },
      "outputs": [],
      "source": [
        "!pipreqs . --force"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}